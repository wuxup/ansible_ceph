################################################################################
#CEPH CONFIG STANDARD                             
#
# This is a standard ceph config file.
#
# Change Log:
# - 2021-11-01	Init by Kern
#
################################################################################

[global]

################################################################################
# DATA PLACEMENT
# https://docs.ceph.com/en/pacific/rados/configuration/pool-pg-config-ref/
osd_pool_default_size = 2

################################################################################
# MONITOR OSD INTERACTION
# https://docs.ceph.com/en/pacific/rados/configuration/mon-osd-interaction/

[mon]

[osd]

################################################################################
# GENERAL SETTINGS
# https://docs.ceph.com/en/pacific/rados/configuration/osd-config-ref/?#general-settings

################################################################################
# FILE SYSTEM SETTINGS

################################################################################
# JOURNAL SETTINGS
# https://docs.ceph.com/en/pacific/rados/configuration/journal-ref/

################################################################################
# SCRUBBING
# https://docs.ceph.com/en/pacific/rados/configuration/osd-config-ref/?#scrubbing

## The maximum number of simultaneous scrub operations for a Ceph OSD Daemon.
osd_max_scrubs = 1

## This restricts scrubbing to this hour of the day or later. 
## Use osd_scrub_begin_hour = 0 and osd_scrub_end_hour = 0 to allow scrubbing the entire day.
## Along with osd_scrub_end_hour, they define a time window, in which the scrubs can happen.
## But a scrub will be performed no matter whether the time window allows or not, as long as
##  the placement group’s scrub interval exceeds osd_scrub_max_interval.
osd_scrub_begin_hour = 0

## This restricts scrubbing to the hour earlier than this. Use osd_scrub_begin_hour = 0 and
##  osd_scrub_end_hour = 0 to allow scrubbing for the entire day. Along with osd_scrub_begin_hour,
##  they define a time window, in which the scrubs can happen. But a scrub will be performed no
##  matter whether the time window allows or not, as long as the placement group’s scrub
##  interval exceeds osd_scrub_max_interval.
osd_scrub_end_hour = 0

## This restricts scrubbing to this day of the week or later. 0 = Sunday, 1 = Monday, etc.
## Use osd_scrub_begin_week_day = 0 and osd_scrub_end_week_day = 0 to allow scrubbing for
##  the entire week. Along with osd_scrub_end_week_day, they define a time window in which
##  scrubs can happen. But a scrub will be performed no matter whether the time window allows
##  or not, when the PG’s scrub interval exceeds osd_scrub_max_interval.
osd_scrub_begin_week_day = 0

## This restricts scrubbing to days of the week earlier than this. 0 = Sunday, 1 = Monday, etc. 
## Use osd_scrub_begin_week_day = 0 and osd_scrub_end_week_day = 0 to allow scrubbing for the
##  entire week. Along with osd_scrub_begin_week_day, they define a time window, in which the scrubs can happen. But a scrub will be performed no matter whether the time window allows or not, as long as the placement group’s scrub interval exceeds osd_scrub_max_interval.
osd_scrub_end_week_day = 0

## Allow scrub during recovery. Setting this to false will disable scheduling new scrub 
## (and deep–scrub) while there is active recovery. Already running scrubs will be continued.
## This might be useful to reduce load on busy clusters.
osd_scrub_during_recovery = false

## The maximum time in seconds before timing out a scrub thread.
osd_scrub_thread_timeout = 60

## The maximum time in seconds before timing out a scrub finalize thread.
osd_scrub_finalize_thread_timeout = 600

## The normalized maximum load. Ceph will not scrub when the system load 
##  (as defined by getloadavg() / number of online CPUs) is higher than this number.
## Default is 0.5.
osd_scrub_load_threshold = 0.5

## The minimal interval in seconds for scrubbing the Ceph OSD Daemon when
##  the Ceph Storage Cluster load is low.
osd_scrub_min_interval = 86400

## The maximum interval in seconds for scrubbing the Ceph OSD Daemon irrespective of cluster load.
osd_scrub_max_interval = 604800

## The minimal number of object store chunks to scrub during single operation.
## Ceph blocks writes to single chunk during scrub.
osd_scrub_chunk_min = 5

## The maximum number of object store chunks to scrub during single operation.
osd_scrub_chunk_max = 25

## Time to sleep before scrubbing the next group of chunks. Increasing this value
##  will slow down the overall rate of scrubbing so that client operations will be less impacted.
osd_scrub_sleep = 0

## The interval for “deep” scrubbing (fully reading all data).
## The osd_scrub_load_threshold does not affect this setting.
osd_deep_scrub_interval = 604800

## Add a random delay to osd_scrub_min_interval when scheduling the next scrub job for a PG.
## The delay is a random value less than osd_scrub_min_interval * osd_scrub_interval_randomized_ratio.
## The default setting spreads scrubs throughout the allowed time window of [1, 1.5] * osd_scrub_min_interval.
osd_scrub_interval_randomize_ratio = 0.5

## Read size when doing a deep scrub.
osd_deep_scrub_stride = 524288

## Setting this to true will enable automatic PG repair when errors are found by scrubs or deep-scrubs.
## However, if more than osd_scrub_auto_repair_num_errors errors are found a repair is NOT performed.
osd_scrub_auto_repair = false

## Auto repair will not occur if more than this many errors are found.
## osd_scrub_auto_repair_num_errors = 5

################################################################################
# OPERATIONS
# https://docs.ceph.com/en/pacific/rados/configuration/osd-config-ref/?#operations

## This selects which priority ops will be sent to the strict queue verses the normal queue.
## osd_op_queue_cut_off

## The priority set for client operations. This value is relative to that of
##  osd_recovery_op_priority below. The default strongly favors client ops over recovery.
osd_client_op_priority = 63

## The priority of recovery operations vs client operations, if not specified by the pool’s
##  recovery_op_priority. The default value prioritizes client ops (see above) over recovery ops.
##  You may adjust the tradeoff of client impact against the time to restore cluster health by lowering
##  this value for increased prioritization of client ops, or by increasing it to favor recovery.
osd_recovery_op_priority = 3

## The default work queue priority for scheduled scrubs when the pool doesn’t specify a value of
##  scrub_priority. This can be boosted to the value of osd_client_op_priority when scrubs are blocking client operations.
osd_scrub_priority = 5

## The priority set for user requested scrub on the work queue. If this value were to be smaller
##  than osd_client_op_priority it can be boosted to the value of osd_client_op_priority when
##   scrub is blocking client operations.
osd_requested_scrub_priority = 120

## The priority set for the snap trim work queue.
## osd_snap_trim_priority = 5

## Time in seconds to sleep before next snap trim op. Increasing this value will slow
##  down snap trimming. This option overrides backend specific variants.
## osd_snap_trim_sleep = 0

## Time in seconds to sleep before next snap trim op for HDDs.
## osd_snap_trim_sleep_hdd = 5

## Time in seconds to sleep before next snap trim op for SSD OSDs (including NVMe).
## osd_snap_trim_sleep_ssd = 0

## Time in seconds to sleep before next snap trim op when OSD data is on an HDD
##  and the OSD journal or WAL+DB is on an SSD.
## osd_snap_trim_sleep_hybrid = 2

## The Ceph OSD Daemon operation thread timeout in seconds.
## osd_op_thread_timeout = 15

## An operation becomes complaint worthy after the specified number of seconds have elapsed.
## osd_op_complaint_time = 30

## The maximum number of completed operations to track.
## osd_op_history_size = 20

## The oldest completed operation to track.
## osd_op_history_duration = 600

## How many operations logs to display at once.
## osd_op_log_threshold = 5

################################################################################
# QOS BASED ON MCLOCK

################################################################################
# BACKFILLING
# https://docs.ceph.com/en/pacific/rados/configuration/osd-config-ref/#backfilling

## The maximum number of backfills allowed to or from a single OSD.
## Note that this is applied separately for read and write operations.
osd_max_backfills = 1

## The minimum number of objects per backfill scan.
osd_backfill_scan_min = 64

## The maximum number of objects per backfill scan.
osd_backfill_scan_max = 512

## The number of seconds to wait before retrying backfill requests.
osd_backfill_retry_interval = 10.0

################################################################################
# OSD MAP
# https://docs.ceph.com/en/pacific/rados/configuration/osd-config-ref/#osd-map

################################################################################
# RECOVERY
# https://docs.ceph.com/en/pacific/rados/configuration/osd-config-ref/#recovery

## After peering completes, Ceph will delay for
##the specified number of seconds before starting to recover RADOS objects
osd_recovery_delay_start = 0

osd_recovery_max_active = 0
## The number of active recovery requests per OSD at one time,
## if the primary device is rotational.
osd_recovery_max_active_hdd = 3

## The number of active recovery requests per OSD at one time,
## if the primary device is non-rotational (i.e., an SSD)
osd_recovery_max_active_ssd = 10

## The maximum size of a recovered chunk of data to push
#osd_recovery_max_chunk = 

## he maximum number of recovery operations per OSD that
## will be newly started when an OSD is recovering.
osd_recovery_max_single_start = 1

## The maximum time in seconds before timing out a recovery thread.
osd_recovery_thread_timeout = 30

## Preserves clone overlap during recovery. Should always be set to true.
osd_recover_clone_overlap = true

## Time in seconds to sleep before the next recovery or backfill op.
## Increasing this value will slow down recovery operation while
## client operations will be less impacted.
osd_recovery_sleep = 0

## Time in seconds to sleep before next recovery or backfill op for HDDs.
osd_recovery_sleep_hdd = 0.1

## Time in seconds to sleep before the next recovery or backfill op for SSDs.
osd_recovery_sleep_ssd = 0

## Time in seconds to sleep before the next recovery or backfill op when
## OSD data is on HDD and OSD journal / WAL+DB is on SSD.
osd_recovery_sleep_hybrid = 0.025

## The default priority set for recovery work queue. Not related to a pool’s recovery_priority.
osd_recovery_priority = 5

################################################################################
# TIERING
# https://docs.ceph.com/en/pacific/rados/configuration/osd-config-ref/#tiering
# BY DEFAULT

################################################################################
# MISCELLANEOUS
# https://docs.ceph.com/en/pacific/rados/configuration/osd-config-ref/#miscellaneous
# BY DEFAULT

[mgr]
mgr_cephadm_device_enhanced_scan = true
